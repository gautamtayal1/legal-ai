---
description:
globs:
alwaysApply: false
---
# üîç **EXTREMELY DETAILED RAG PIPELINE FLOW FOR LEGAL AI**

## üéØ **What is RAG First?**
**RAG = Retrieval-Augmented Generation** = Instead of asking ChatGPT to guess answers, we first **retrieve** relevant information from your documents, then use that information to **generate** accurate, cited answers.

---

## üìã **COMPLETE TOOLS & TECHNOLOGIES LIST**

### **Text Extraction**
- **PDF**: PyPDF2, pdfplumber, pymupdf, pdfminer, Adobe PDF Extract API
- **DOCX**: python-docx, docx2txt, mammoth
- **OCR**: Tesseract, AWS Textract, Google Document AI, Azure Form Recognizer

### **Text Processing & Chunking**
- **Chunking**: LangChain TextSplitter, semantic-chunker, spaCy sentence segmentation
- **Text Cleaning**: regex, NLTK, spaCy, BeautifulSoup

### **Vector Databases (Embeddings Storage)**
- **Cloud**: Pinecone, Weaviate, Qdrant Cloud, Supabase Vector
- **Self-hosted**: Chroma, FAISS, Weaviate, Qdrant, Milvus, pgvector (PostgreSQL extension)

### **Embedding Models**
- **OpenAI**: text-embedding-ada-002, text-embedding-3-small/large
- **Open Source**: sentence-transformers, all-MiniLM-L6-v2, BGE, E5
- **Specialized**: Cohere embeddings, HuggingFace transformers

### **Search Engines (Keyword Search)**
- **Elasticsearch**: Most popular, powerful querying
- **Alternatives**: Apache Solr, OpenSearch, Typesense, Algolia

### **Background Job Processing**
- **Python**: Celery + Redis, RQ (Redis Queue), TaskiQ
- **Alternatives**: Apache Airflow, Prefect

### **Large Language Models (Answer Generation)**
- **APIs**: OpenAI GPT-4, Claude, Gemini, Cohere
- **Self-hosted**: Llama 2/3, Mistral, Code Llama, Phi-3

---

## üåä **SINGLE LINE MEGA-DETAILED FLOW**

```
[USER UPLOADS LEGAL DOCUMENT] ‚Üí [FILE VALIDATION & S3 STORAGE] ‚Üí [BACKGROUND JOB TRIGGERS] ‚Üí [TEXT EXTRACTION FROM FILE] ‚Üí [DOCUMENT STRUCTURE ANALYSIS] ‚Üí [LEGAL CONTENT EXTRACTION] ‚Üí [INTELLIGENT LEGAL CHUNKING] ‚Üí [GENERATE VECTOR EMBEDDINGS] ‚Üí [STORE IN VECTOR DATABASE] ‚Üí [INDEX IN ELASTICSEARCH] ‚Üí [STORE METADATA IN POSTGRESQL] ‚Üí [MARK PROCESSING COMPLETE] ‚Üí ‚Ä¢‚Ä¢‚Ä¢ [USER ASKS LEGAL QUESTION] ‚Ä¢‚Ä¢‚Ä¢ ‚Üí [QUERY PREPROCESSING] ‚Üí [ROUND 1: SEMANTIC + KEYWORD SEARCH] ‚Üí [ROUND 2: CROSS-REFERENCE EXPANSION] ‚Üí [ROUND 3: DEFINITION RETRIEVAL] ‚Üí [CONTEXT AGGREGATION & RANKING] ‚Üí [LEGAL OBLIGATION ANALYSIS] ‚Üí [NUMERICAL CONSISTENCY CHECK] ‚Üí [CITATION PREPARATION] ‚Üí [LLM ANSWER GENERATION] ‚Üí [RESPONSE FORMATTING] ‚Üí [FRONTEND DISPLAY WITH CITATIONS]
```

---

## üîç **STEP-BY-STEP BREAKDOWN WITH EXTREME DETAIL**

### **STEP 1: [USER UPLOADS LEGAL DOCUMENT]**
**What happens**: User clicks "Upload" button on frontend and selects contract.pdf file
**Technical details**: Frontend uses HTML file input, converts to FormData, sends POST request to `/documents/upload`
**Tools involved**: React/Next.js file upload component, browser File API
**Output**: File object sent to backend API

---

### **STEP 2: [FILE VALIDATION & S3 STORAGE]**
**What happens**: Backend validates file type (PDF/DOCX/TXT), checks size limit (10MB), uploads to AWS S3 bucket, generates unique key
**Why S3**: Cloud storage that can handle massive files, provides public URLs, integrates with other AWS services
**Technical details**: FastAPI receives UploadFile, reads bytes into memory, boto3 uploads to S3 with unique UUID-based key
**Tools involved**: FastAPI UploadFile, boto3 (AWS SDK), AWS S3
**Database record**: Creates Document row in PostgreSQL with status="pending", stores S3 URL and metadata
**Output**: S3 URL like `https://bucket.s3.region.amazonaws.com/uploads/uuid_contract.pdf`

---

### **STEP 3: [BACKGROUND JOB TRIGGERS]**
**What happens**: Instead of making user wait, system triggers asynchronous background processing job and immediately returns success to user
**Why background**: Document processing takes 30 seconds to 5 minutes depending on size - user shouldn't wait
**Technical details**: Redis queue receives job with document ID, Celery worker picks up job for processing
**Tools involved**: Redis (message broker), Celery (distributed task queue), or RQ (simpler alternative)
**Database update**: Document status changes to "processing"
**Output**: Job ID for tracking, user sees "Processing..." status

---

### **STEP 4: [TEXT EXTRACTION FROM FILE]**
**What happens**: Background worker downloads file from S3, uses appropriate library to extract raw text content based on file type
**For PDF files**: PyPDF2 or pdfplumber reads PDF structure, extracts text from each page, preserves formatting where possible
**For DOCX files**: python-docx library reads Microsoft Word XML structure, extracts paragraphs, headings, and formatting
**For TXT files**: Simple file reading with encoding detection (UTF-8, ASCII, etc.)
**OCR fallback**: If PDF is scanned (image-based), use Tesseract or AWS Textract to convert images to text
**Technical details**: Download file bytes from S3, detect file type, instantiate appropriate parser, extract text string
**Tools involved**: PyPDF2/pdfplumber (PDF), python-docx (Word), chardet (encoding detection), Tesseract (OCR)
**Quality checks**: Verify text extraction worked (not empty, reasonable length), log any errors
**Output**: Raw text string containing entire document content

---

### **STEP 5: [DOCUMENT STRUCTURE ANALYSIS]**
**What happens**: Analyze the extracted text to understand document structure - identify sections, subsections, headers, page numbers
**Legal document patterns**: Look for "Section 1", "Article I", "1.1", numbered lists, defined terms sections, signature blocks
**Technical details**: Use regex patterns to identify section headers, build hierarchical tree of document structure
**Example patterns**: `Section \d+`, `\d+\.\d+`, `Article [IVX]+`, `WHEREAS,`, `NOW THEREFORE`
**Tools involved**: Regular expressions (re module), spaCy for sentence segmentation, custom legal document parsers
**Database storage**: Store document hierarchy in PostgreSQL as parent-child relationships between sections
**Output**: Structured representation like `{"section_1": {"subsection_1_1": {...}, "subsection_1_2": {...}}}`

---

### **STEP 6: [LEGAL CONTENT EXTRACTION]**
**What happens**: Extract legal-specific content like definitions, cross-references, obligations, and key terms

**6A: DEFINITION EXTRACTION**
**What it does**: Find the "Definitions" section (usually early in contract), extract all defined terms and their meanings
**Pattern matching**: Look for `"Term" means...`, `Term shall mean...`, `For purposes of this Agreement, "X" means...`
**Technical details**: Identify definitions section, parse term-definition pairs, create mapping dictionary
**Tools involved**: Regex patterns, NLP libraries like spaCy for entity recognition
**Example output**: `{"Effective Date": "January 1, 2024", "Confidential Information": "any non-public information..."}`

**6B: CROSS-REFERENCE EXTRACTION**
**What it does**: Find all references to other parts of the document like "see Section 5.2", "as defined in Article III"
**Pattern matching**: `Section \d+\.\d+`, `Article [IVX]+`, `paragraph (a)`, `subsection \d+`
**Purpose**: Later when user asks about Section 2, we automatically include referenced Section 5.2
**Tools involved**: Regex for reference patterns, graph structures to map relationships
**Example output**: `{"section_2_1": ["section_5_2", "article_3"], "section_4": ["definitions"]}`

**6C: OBLIGATION EXTRACTION**
**What it does**: Find who must do what and when - critical for legal analysis
**Pattern matching**: `[Party] shall...`, `[Party] must...`, `[Party] will...`, `within X days`
**Technical details**: Identify parties (Company, Client, etc.), extract obligations, find deadlines
**Tools involved**: Named Entity Recognition (NER), dependency parsing with spaCy
**Example output**: `{"party": "Company", "obligation": "deliver reports", "deadline": "30 days"}`

---

### **STEP 7: [INTELLIGENT LEGAL CHUNKING]**
**What happens**: Break document into smaller pieces while preserving legal meaning and context

**Why chunking is needed**: 
- Vector databases work best with smaller text pieces (200-500 tokens)
- LLMs have context limits (can't process entire 50-page contract at once)
- Better retrieval precision (find exact relevant paragraphs, not entire sections)

**Legal-specific chunking rules**:
- **Never split mid-sentence** (legal language is precise, context matters)
- **Keep related clauses together** (don't separate "Company shall..." from "...within 30 days")
- **Preserve section context** (include section headers with chunks)
- **Maintain numbering** (keep "5.2.1" with its content)

**Technical process**:
1. Use sentence segmentation to identify sentence boundaries
2. Group sentences into chunks of ~300-500 tokens
3. Add section header as context to each chunk
4. Create overlap between chunks (50-100 tokens) to avoid losing context at boundaries
5. Add metadata: section_path, chunk_index, document_id, character_positions

**Tools involved**: LangChain TextSplitter, spaCy sentence segmentation, tiktoken for token counting
**Example chunk**: 
```
Chunk 387: "Section 5.2 Termination Rights. Company may terminate this Agreement upon thirty (30) days written notice to Client if Client fails to pay any amount when due..."
Metadata: {section: "5.2", tokens: 287, overlap_start: 50, overlap_end: 45}
```
**Output**: List of 50-200 chunks per document, each with text content and rich metadata

---

### **STEP 8: [GENERATE VECTOR EMBEDDINGS]**
**What happens**: Convert each text chunk into a mathematical vector representation that captures semantic meaning

**What are embeddings**: Numbers that represent meaning - similar concepts have similar numbers
**Example**: "termination", "ending", "cancellation" would have very similar vector values
**Why vectors**: Computers can calculate similarity between vectors to find related content

**Technical process**:
1. Send each chunk text to embedding model API (OpenAI, Cohere, etc.)
2. Receive back array of 1536 numbers (for OpenAI ada-002) representing that chunk's meaning
3. Store vector alongside chunk metadata

**Embedding model options**:
- **OpenAI text-embedding-ada-002**: $0.0001 per 1K tokens, very good quality
- **OpenAI text-embedding-3-large**: Newer, better performance, higher cost
- **Open source alternatives**: sentence-transformers models (free but need GPU)

**Tools involved**: OpenAI API, transformers library, numpy for vector operations
**API call example**: `openai.embeddings.create(input="chunk text", model="text-embedding-ada-002")`
**Output**: Vector like `[0.123, -0.456, 0.789, ...]` with 1536 dimensions per chunk

---

### **STEP 9: [STORE IN VECTOR DATABASE]**
**What happens**: Store vectors in specialized database optimized for similarity search

**What vector databases do**: Store millions of vectors, find most similar vectors to query vector in milliseconds
**How similarity works**: Calculate cosine similarity between query vector and stored vectors
**Why not regular database**: PostgreSQL can't efficiently search through millions of high-dimensional vectors

**Storage process**:
1. Connect to vector database (Pinecone, Weaviate, etc.)
2. Create namespace/collection for this document
3. Insert each chunk with: vector, text content, metadata (document_id, section, chunk_id)
4. Build optimized index for fast similarity search

**Vector database options**:
- **Pinecone**: Managed cloud service, easiest to use, pay-per-vector
- **Weaviate**: Open source or cloud, good performance, GraphQL API
- **Chroma**: Simple local database, good for development
- **pgvector**: PostgreSQL extension, keep everything in one database

**Tools involved**: Pinecone client, Weaviate client, or database-specific SDK
**Example storage**: 
```
{
  "id": "doc_123_chunk_387",
  "vector": [0.123, -0.456, ...],
  "metadata": {"document_id": 123, "section": "5.2", "text": "Section 5.2 Termination..."}
}
```
**Output**: All chunks indexed and searchable by semantic similarity

---

### **STEP 10: [INDEX IN ELASTICSEARCH]**
**What happens**: Store chunk text in Elasticsearch for precise keyword and phrase searching

**Why Elasticsearch**: Vector search finds semantically similar content, but sometimes you need exact keyword matches
**Legal example**: User searches for "30 days" - must find exact phrase, not "one month" or "thirty days"
**Complement to vectors**: Hybrid search combines semantic similarity (vectors) + exact keywords (Elasticsearch)

**Indexing process**:
1. Connect to Elasticsearch cluster
2. Create index with proper mappings for legal text
3. Insert each chunk with full text content and metadata
4. Configure analyzers for legal language (preserve numbers, dates, legal terms)

**Elasticsearch features used**:
- **Full-text search**: Find exact phrases across all chunks
- **Fuzzy matching**: Handle typos and variations
- **Boolean queries**: Complex searches like "termination AND (30 days OR sixty days)"
- **Highlighting**: Show exactly where search terms were found

**Tools involved**: elasticsearch-py client, Elasticsearch cluster (cloud or self-hosted)
**Index configuration**: 
```json
{
  "mappings": {
    "properties": {
      "text": {"type": "text", "analyzer": "legal_analyzer"},
      "section": {"type": "keyword"},
      "document_id": {"type": "integer"}
    }
  }
}
```
**Output**: Full-text searchable index of all document chunks

---

### **STEP 11: [STORE METADATA IN POSTGRESQL]**
**What happens**: Store structured relationships, processing status, and chunk metadata in relational database

**Why PostgreSQL**: Vector and Elasticsearch store content, but PostgreSQL stores relationships and business logic
**What gets stored**:
- Document processing status and timestamps
- Chunk hierarchy and relationships
- Cross-reference mappings
- Definition catalog
- User query history

**Database tables created**:
- `document_chunks`: Each chunk with position, section, token count
- `document_definitions`: Extracted terms and their definitions  
- `document_references`: Cross-reference mappings between sections
- `processing_jobs`: Background job status and logs

**Tools involved**: SQLAlchemy ORM, PostgreSQL database, Alembic for migrations
**Example records**:
```sql
INSERT INTO document_chunks (document_id, chunk_index, section_path, text_content, token_count, vector_id)
VALUES (123, 387, 'section_5_2', 'Section 5.2 Termination Rights...', 287, 'doc_123_chunk_387');
```
**Output**: Structured metadata enabling complex queries and relationship tracking

---

### **STEP 12: [MARK PROCESSING COMPLETE]**
**What happens**: Update document status, log processing results, notify user of completion

**Status update**: Change document.processing_status from "processing" to "completed" (or "failed" if errors)
**Metrics logged**: Processing time, number of chunks created, any errors encountered
**User notification**: Frontend polls status API or receives WebSocket notification
**Quality checks**: Verify all chunks were processed, embeddings generated, searches working

**Tools involved**: Database update, logging, WebSocket or polling for real-time updates
**Output**: Document ready for querying, user can start asking questions

---

### **‚Ä¢‚Ä¢‚Ä¢ [USER ASKS LEGAL QUESTION] ‚Ä¢‚Ä¢‚Ä¢**
**What happens**: User types question like "What are my termination rights?" in chat interface
**Frontend**: React chat component sends POST request to `/chat/query` endpoint
**Backend receives**: Question text, document ID(s) to search, user context

---

### **STEP 13: [QUERY PREPROCESSING]**
**What happens**: Analyze and prepare user question for optimal search

**Query analysis**:
- **Intent detection**: Is this about obligations, definitions, deadlines, parties?
- **Entity extraction**: Find legal terms, party names, section references in question
- **Query expansion**: Add synonyms ("termination" ‚Üí "ending", "cancellation", "expiration")
- **Legal context**: Understand this is a legal query requiring precise, cited answers

**Technical process**:
1. Extract entities using NER (Named Entity Recognition)
2. Identify legal concepts and terminology
3. Generate search variations and synonyms
4. Prepare query for different search types

**Tools involved**: spaCy NER, legal term dictionaries, query expansion algorithms
**Example preprocessing**:
- Input: "What are my termination rights?"
- Processed: ["termination rights", "termination clauses", "ending agreement", "cancellation rights"]
**Output**: Enhanced query ready for multi-round search

---

### **STEP 14: [ROUND 1: SEMANTIC + KEYWORD SEARCH]**
**What happens**: First retrieval round combining vector similarity search and keyword matching

**14A: SEMANTIC SEARCH (Vector Database)**
**Process**: 
1. Convert user question to embedding vector using same model as chunks
2. Search vector database for most similar chunks (cosine similarity)
3. Return top 10-20 most semantically similar chunks
**Example**: "termination rights" finds chunks about "ending agreement", "contract cancellation" even without exact words

**14B: KEYWORD SEARCH (Elasticsearch)**
**Process**:
1. Search Elasticsearch for exact phrase matches
2. Use boolean queries for precision ("termination" AND "rights")
3. Find chunks containing exact legal terminology
**Example**: Finds exact phrase "terminate this Agreement" that semantic search might miss

**14C: RESULT COMBINATION**
**Process**:
1. Merge results from both searches
2. Remove duplicates
3. Score chunks by relevance (semantic similarity + keyword match strength)
4. Rank by combined score

**Tools involved**: Vector database client, Elasticsearch client, similarity scoring algorithms
**Search APIs**:
```python
# Vector search
vector_results = pinecone.query(vector=query_embedding, top_k=20)
# Keyword search  
keyword_results = es.search(body={"query": {"match": {"text": "termination rights"}}})
```
**Output**: 15-25 relevant chunks ranked by relevance score

---

### **STEP 15: [ROUND 2: CROSS-REFERENCE EXPANSION]**
**What happens**: Automatically follow references found in Round 1 results to gather complete context

**Reference detection**: Scan retrieved chunks for patterns like "see Section 5.2", "as defined in Article III"
**Automatic fetching**: For each reference found, automatically retrieve the referenced section
**Why critical for legal**: Legal documents are highly interconnected - understanding one clause often requires reading referenced clauses

**Technical process**:
1. Use regex to find reference patterns in Round 1 chunks
2. Look up section mappings in PostgreSQL database  
3. Retrieve referenced chunks from vector database or Elasticsearch
4. Include parent/child sections if helpful for context

**Tools involved**: Regular expressions, database queries, recursive chunk retrieval
**Example**:
- Round 1 finds: "Company may terminate as provided in Section 8.2"
- Round 2 automatically fetches: Section 8.2 content about termination procedures
**Output**: Original chunks + all referenced sections for complete legal context

---

### **STEP 16: [ROUND 3: DEFINITION RETRIEVAL]**
**What happens**: Identify defined terms in all retrieved chunks and fetch their definitions

**Term identification**: Scan chunks for capitalized terms that were extracted during document processing
**Definition lookup**: Query definitions table for exact definitions from the document
**Context inclusion**: Add definitions to retrieval context so LLM understands specialized terms

**Technical process**:
1. Extract potential defined terms from retrieved text (capitalized phrases, quoted terms)
2. Query document_definitions table for matches
3. Retrieve definition text and context
4. Add definitions to final context payload

**Tools involved**: NER for term extraction, PostgreSQL queries, text matching algorithms
**Example**:
- Retrieved chunk mentions: "Confidential Information"
- Round 3 fetches: "Confidential Information means any non-public, proprietary information..."
**Output**: Complete context including all relevant definitions

---

### **STEP 17: [CONTEXT AGGREGATION & RANKING]**
**What happens**: Combine all retrieved information, remove duplicates, and create final context for LLM

**Aggregation process**:
1. Merge chunks from all three rounds
2. Remove exact duplicates 
3. Remove low-relevance chunks if context is too large
4. Organize by document structure (keep related sections together)
5. Ensure context fits within LLM token limits

**Ranking factors**:
- Semantic similarity score from Round 1
- Keyword match strength
- Section importance (definitions and main clauses rank higher)
- Cross-reference connectivity (frequently referenced sections rank higher)

**Context optimization**:
- Prioritize chunks that directly answer the question
- Include necessary context for understanding
- Trim to fit LLM context window (typically 8K-32K tokens)

**Tools involved**: Deduplication algorithms, relevance scoring, text summarization if needed
**Output**: Ranked, deduplicated context optimized for answer generation

---

### **STEP 18: [LEGAL OBLIGATION ANALYSIS]**
**What happens**: Analyze retrieved context to identify legal obligations, parties, conditions, and requirements

**Obligation extraction**:
- Find "shall", "must", "will" statements indicating legal requirements
- Identify parties responsible for each obligation
- Extract conditions and triggers ("if", "unless", "provided that")
- Find deadlines and time requirements

**Party identification**: 
- Map generic terms ("Company", "Client") to actual entity names
- Track which party has which obligations
- Identify mutual obligations vs. one-sided requirements

**Condition analysis**:
- Find conditional obligations ("Company shall X if Y occurs")
- Identify triggering events and circumstances
- Map obligation dependencies and sequences

**Tools involved**: NLP for obligation detection, legal pattern matching, dependency parsing
**Example analysis**:
```json
{
  "obligations": [
    {
      "party": "Company", 
      "action": "provide 30 days written notice",
      "condition": "if terminating for cause",
      "deadline": "30 days before termination"
    }
  ]
}
```
**Output**: Structured legal analysis of obligations and requirements

---

### **STEP 19: [NUMERICAL CONSISTENCY CHECK]**
**What happens**: Scan all retrieved content for numbers, dates, and deadlines to identify potential conflicts

**Number extraction**: Find all numerical values (amounts, percentages, days, dates) in retrieved chunks
**Conflict detection**: Identify cases where same concept has different numbers in different clauses
**Critical for legal**: Contracts sometimes have inconsistent deadlines or amounts that need flagging

**Technical process**:
1. Extract numbers using regex and NER
2. Categorize by type (monetary amounts, time periods, percentages)
3. Group by concept (e.g., all "termination notice" periods)
4. Flag inconsistencies for user attention

**Tools involved**: Regular expressions, number extraction, conflict detection algorithms
**Example conflicts found**:
- Section 3: "30 days notice required"
- Section 7: "60 days notice required"  
- **Flag**: Conflicting notice periods
**Output**: List of numerical conflicts and inconsistencies for user awareness

---

### **STEP 20: [CITATION PREPARATION]**
**What happens**: Prepare precise citations for every fact that will appear in the answer

**Citation mapping**: Link each piece of information back to its exact source chunk and document location
**Clickable citations**: Prepare data for frontend to create clickable citation links
**Source verification**: Ensure every claim can be traced back to specific document text

**Citation format preparation**:
- Map facts to chunk IDs and character positions
- Prepare section references (e.g., "Section 5.2, paragraph 3")
- Generate unique citation IDs for frontend linking

**Tools involved**: Citation tracking, metadata mapping, reference formatting
**Example citation data**:
```json
{
  "fact": "30 days written notice required",
  "citation_id": "cite_1",
  "source_chunk": "doc_123_chunk_387", 
  "section": "Section 5.2",
  "char_start": 245,
  "char_end": 289
}
```
**Output**: Complete citation mapping for answer verification

---

### **STEP 21: [LLM ANSWER GENERATION]**
**What happens**: Send all retrieved context and analysis to Large Language Model for comprehensive answer generation

**Prompt engineering**: Create carefully crafted prompt that includes:
- User's original question
- All retrieved and analyzed context
- Instructions for legal accuracy and citation requirements
- Format requirements for structured response

**LLM processing**: 
- GPT-4 or Claude analyzes all provided context
- Generates comprehensive answer based solely on provided information
- Includes proper legal reasoning and analysis
- Formats response with citations and structure

**Quality controls**:
- Instruct LLM to only use provided context (no hallucination)
- Require citations for every factual claim
- Request structured format with clear sections
- Ask for confidence indicators where appropriate

**Tools involved**: OpenAI API, Claude API, or other LLM service, prompt templates
**Example prompt structure**:
```
You are a legal AI assistant. Based ONLY on the provided context, answer this question: "What are my termination rights?"

Context: [All retrieved chunks and analysis]

Requirements:
- Cite every fact with [cite_X] format
- Structure answer clearly
- Only use provided information
- Indicate if information is incomplete
```
**Output**: Comprehensive, cited legal answer generated by LLM

---

### **STEP 22: [RESPONSE FORMATTING]**
**What happens**: Structure the LLM-generated answer with proper formatting, citations, and metadata for frontend display

**Response structuring**:
- Parse LLM response for citation markers
- Map citation markers to prepared citation data
- Format answer sections (summary, detailed analysis, caveats)
- Add metadata about search process and confidence

**Citation linking**: 
- Replace citation markers with clickable citation objects
- Include chunk IDs for frontend to enable document jumping
- Add tooltip text with exact quoted material

**Metadata addition**:
- Search process summary ("Retrieved 23 chunks across 3 rounds")
- Confidence indicators where analysis is incomplete
- Suggestions for follow-up questions

**Tools involved**: Text parsing, citation mapping, response formatting
**Example formatted response**:
```json
{
  "answer": "You have three termination rights: [cite_1] immediate termination for material breach, [cite_2] termination with 30 days notice...",
  "citations": [
    {
      "id": "cite_1",
      "text": "Company may terminate immediately upon material breach",
      "source": "Section 8.1",
      "chunk_id": "doc_123_chunk_445"
    }
  ],
  "metadata": {
    "chunks_retrieved": 23,
    "confidence": "high",
    "search_rounds": 3
  }
}
```
**Output**: Fully formatted response ready for frontend display

---

### **STEP 23: [FRONTEND DISPLAY WITH CITATIONS]**
**What happens**: Present the answer to user with interactive citations and document viewing capabilities

**Answer display**:
- Show main answer with inline citation links
- Provide expandable sections for detailed analysis
- Display search process metadata if user wants to see "how you found this"

**Citation interactivity**:
- Clickable citations that highlight exact source text
- Tooltip previews of cited content
- "Jump to document" functionality to view full context

**Document viewer integration**:
- Side-by-side document viewer showing original contract
- Automatic scrolling to cited sections when citations clicked
- Highlighting of relevant passages in original document

**Follow-up capabilities**:
- Suggested follow-up questions based on answer
- "Ask more about this section" options
- Conversation context maintained for deeper exploration

**Tools involved**: React components, PDF viewer libraries, citation linking, WebSocket for real-time updates
**User experience**: User sees comprehensive answer with full traceability back to source document
**Output**: Interactive legal answer with complete source verification

---

## üéØ **FINAL RESULT FOR USER**

**User asked**: "What are my termination rights?"

**User receives**: 
- Comprehensive answer covering all termination scenarios
- Every fact linked to specific contract sections  
- Ability to click any citation to see exact source text
- Clear explanation of conditions, deadlines, and requirements
- Confidence indicators and caveats where appropriate
- Suggestions for related questions

**Time**: 2-5 seconds instead of 30+ minutes of manual contract review
**Accuracy**: Higher than manual review due to systematic analysis
**Verifiability**: Every claim traceable to exact source location

This RAG pipeline transforms legal document analysis from manual, error-prone process into instant, systematic, verifiable answers with complete source citation.