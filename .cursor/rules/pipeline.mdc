---
description:
globs:
alwaysApply: false
---
# üîç **EXTREMELY DETAILED RAG PIPELINE FLOW FOR LEGAL AI**

## üéØ **What is RAG First?**
**RAG = Retrieval-Augmented Generation** = Instead of asking ChatGPT to guess answers, we first **retrieve** relevant information from your documents, then use that information to **generate** accurate, cited answers.

---

## üìã **COMPLETE TOOLS & TECHNOLOGIES LIST**

### **Text Extraction**
- **PDF**: PyPDF2, pdfplumber, pymupdf, pdfminer, Adobe PDF Extract API
- **DOCX**: python-docx, docx2txt, mammoth
- **OCR**: Tesseract, AWS Textract, Google Document AI, Azure Form Recognizer

### **Text Processing & Chunking**
- **Chunking**: LangChain TextSplitter, semantic-chunker, spaCy sentence segmentation
- **Text Cleaning**: regex, NLTK, spaCy, BeautifulSoup

### **Vector Databases (Embeddings Storage)**
- **Cloud**: Pinecone, Weaviate, Qdrant Cloud, Supabase Vector
- **Self-hosted**: Chroma, FAISS, Weaviate, Qdrant, Milvus, pgvector (PostgreSQL extension)

### **Embedding Models**
- **OpenAI**: text-embedding-ada-002, text-embedding-3-small/large
- **Open Source**: sentence-transformers, all-MiniLM-L6-v2, BGE, E5
- **Specialized**: Cohere embeddings, HuggingFace transformers

### **Search Engines (Keyword Search)**
- **Elasticsearch**: Most popular, powerful querying
- **Alternatives**: Apache Solr, OpenSearch, Typesense, Algolia

### **Background Job Processing**
- **Python**: Celery + Redis, RQ (Redis Queue), TaskiQ
- **Alternatives**: Apache Airflow, Prefect

### **Large Language Models (Answer Generation)**
- **APIs**: OpenAI GPT-4, Claude, Gemini, Cohere
- **Self-hosted**: Llama 2/3, Mistral, Code Llama, Phi-3

---

## üåä **SINGLE LINE MEGA-DETAILED FLOW**

```
[USER UPLOADS LEGAL DOCUMENT] ‚Üí [FILE VALIDATION & S3 STORAGE] ‚Üí [BACKGROUND JOB TRIGGERS] ‚Üí [TEXT EXTRACTION FROM FILE] ‚Üí [BASIC TEXT PREPROCESSING] ‚Üí [INTELLIGENT LEGAL CHUNKING] ‚Üí [GENERATE VECTOR EMBEDDINGS] ‚Üí [STORE IN VECTOR DATABASE] ‚Üí [INDEX IN ELASTICSEARCH] ‚Üí [STORE METADATA IN POSTGRESQL] ‚Üí [MARK PROCESSING COMPLETE] ‚Üí ‚Ä¢‚Ä¢‚Ä¢ [USER ASKS LEGAL QUESTION] ‚Ä¢‚Ä¢‚Ä¢ ‚Üí [QUERY PREPROCESSING] ‚Üí [SEMANTIC + KEYWORD SEARCH] ‚Üí [LLM ANSWER GENERATION] ‚Üí [RESPONSE FORMATTING] ‚Üí [FRONTEND DISPLAY WITH CITATIONS]
```

---

## üîç **STEP-BY-STEP BREAKDOWN WITH EXTREME DETAIL**

### **STEP 1: [USER UPLOADS LEGAL DOCUMENT]**
**What happens**: User clicks "Upload" button on frontend and selects contract.pdf file
**Technical details**: Frontend uses HTML file input, converts to FormData, sends POST request to `/documents/upload`
**Tools involved**: React/Next.js file upload component, browser File API
**Output**: File object sent to backend API

---

### **STEP 2: [FILE VALIDATION & S3 STORAGE]**
**What happens**: Backend validates file type (PDF/DOCX/TXT), checks size limit (10MB), uploads to AWS S3 bucket, generates unique key
**Why S3**: Cloud storage that can handle massive files, provides public URLs, integrates with other AWS services
**Technical details**: FastAPI receives UploadFile, reads bytes into memory, boto3 uploads to S3 with unique UUID-based key
**Tools involved**: FastAPI UploadFile, boto3 (AWS SDK), AWS S3
**Database record**: Creates Document row in PostgreSQL with status="pending", stores S3 URL and metadata
**Output**: S3 URL like `https://bucket.s3.region.amazonaws.com/uploads/uuid_contract.pdf`

---

### **STEP 3: [BACKGROUND JOB TRIGGERS]**
**What happens**: Instead of making user wait, system triggers asynchronous background processing job and immediately returns success to user
**Why background**: Document processing takes 30 seconds to 5 minutes depending on size - user shouldn't wait
**Technical details**: Redis queue receives job with document ID, Celery worker picks up job for processing
**Tools involved**: Redis (message broker), Celery (distributed task queue), or RQ (simpler alternative)
**Database update**: Document status changes to "processing"
**Output**: Job ID for tracking, user sees "Processing..." status

---

### **STEP 4: [TEXT EXTRACTION FROM FILE]**
**What happens**: Background worker downloads file from S3, uses appropriate library to extract raw text content based on file type
**For PDF files**: PyPDF2 or pdfplumber reads PDF structure, extracts text from each page, preserves formatting where possible
**For DOCX files**: python-docx library reads Microsoft Word XML structure, extracts paragraphs, headings, and formatting
**For TXT files**: Simple file reading with encoding detection (UTF-8, ASCII, etc.)
**OCR fallback**: If PDF is scanned (image-based), use Tesseract or AWS Textract to convert images to text
**Technical details**: Download file bytes from S3, detect file type, instantiate appropriate parser, extract text string
**Tools involved**: PyPDF2/pdfplumber (PDF), python-docx (Word), chardet (encoding detection), Tesseract (OCR)
**Quality checks**: Verify text extraction worked (not empty, reasonable length), log any errors
**Output**: Raw text string containing entire document content

---

### **STEP 5: [BASIC TEXT PREPROCESSING]**
**What happens**: Clean and prepare text for chunking - remove extra whitespace, handle encoding issues
**Technical details**: Basic text normalization, encoding detection, line break handling
**Tools involved**: Basic string processing, chardet for encoding
**Output**: Clean text ready for chunking

---

### **STEP 6: [INTELLIGENT LEGAL CHUNKING]**
**What happens**: Break document into smaller pieces while preserving legal meaning and context

**Why chunking is needed**: 
- Vector databases work best with smaller text pieces (200-500 tokens)
- LLMs have context limits (can't process entire 50-page contract at once)
- Better retrieval precision (find exact relevant paragraphs, not entire sections)

**Legal-specific chunking rules**:
- **Never split mid-sentence** (legal language is precise, context matters)
- **Keep related clauses together** (don't separate "Company shall..." from "...within 30 days")
- **Preserve section context** (include section headers with chunks)
- **Maintain numbering** (keep "5.2.1" with its content)

**Technical process**:
1. Use sentence segmentation to identify sentence boundaries
2. Group sentences into chunks of ~300-500 tokens
3. Add section header as context to each chunk
4. Create overlap between chunks (50-100 tokens) to avoid losing context at boundaries
5. Add metadata: section_path, chunk_index, document_id, character_positions

**Tools involved**: LangChain TextSplitter, spaCy sentence segmentation, tiktoken for token counting
**Example chunk**: 
```
Chunk 387: "Section 5.2 Termination Rights. Company may terminate this Agreement upon thirty (30) days written notice to Client if Client fails to pay any amount when due..."
Metadata: {section: "5.2", tokens: 287, overlap_start: 50, overlap_end: 45}
```
**Output**: List of 50-200 chunks per document, each with text content and rich metadata

---

### **STEP 7: [GENERATE VECTOR EMBEDDINGS]**
**What happens**: Convert each text chunk into a mathematical vector representation that captures semantic meaning

**What are embeddings**: Numbers that represent meaning - similar concepts have similar numbers
**Example**: "termination", "ending", "cancellation" would have very similar vector values
**Why vectors**: Computers can calculate similarity between vectors to find related content

**Technical process**:
1. Send each chunk text to embedding model API (OpenAI)
2. Receive back array of 1536 numbers (for OpenAI ada-002) representing that chunk's meaning
3. Store vector alongside chunk metadata

**API call example**: `openai.embeddings.create(input="chunk text", model="text-embedding-ada-002")`
**Output**: Vector like `[0.123, -0.456, 0.789, ...]` with 1536 dimensions per chunk

---

### **STEP 8: [STORE IN VECTOR DATABASE]**
**What happens**: Store vectors in specialized database optimized for similarity search

**What vector databases do**: Store millions of vectors, find most similar vectors to query vector in milliseconds
**How similarity works**: Calculate cosine similarity between query vector and stored vectors
**Why not regular database**: PostgreSQL can't efficiently search through millions of high-dimensional vectors

**Storage process**:
1. Connect to vector database (ChromaDB)
2. Create namespace/collection for this document
3. Insert each chunk with: vector, text content, metadata (document_id, section, chunk_id)
4. Build optimized index for fast similarity search

**Tools involved**: ChromaDB client

**Example storage**: 
```
{
  "id": "doc_123_chunk_387",
  "vector": [0.123, -0.456, ...],
  "metadata": {"document_id": 123, "section": "5.2", "text": "Section 5.2 Termination..."}
}
```
**Output**: All chunks indexed and searchable by semantic similarity

---

### **STEP 9: [INDEX IN ELASTICSEARCH]**
**What happens**: Store chunk text in Elasticsearch for precise keyword and phrase searching

**Why Elasticsearch**: Vector search finds semantically similar content, but sometimes you need exact keyword matches
**Legal example**: User searches for "30 days" - must find exact phrase, not "one month" or "thirty days"
**Complement to vectors**: Hybrid search combines semantic similarity (vectors) + exact keywords (Elasticsearch)

**Indexing process**:
1. Connect to Elasticsearch cluster
2. Create index with proper mappings for legal text
3. Insert each chunk with full text content and metadata
4. Configure analyzers for legal language (preserve numbers, dates, legal terms)

**Elasticsearch features used**:
- **Full-text search**: Find exact phrases across all chunks
- **Fuzzy matching**: Handle typos and variations
- **Boolean queries**: Complex searches like "termination AND (30 days OR sixty days)"
- **Highlighting**: Show exactly where search terms were found

**Tools involved**: elasticsearch-py client, Elasticsearch cluster (cloud or self-hosted)
**Index configuration**: 
```json
{
  "mappings": {
    "properties": {
      "text": {"type": "text", "analyzer": "legal_analyzer"},
      "section": {"type": "keyword"},
      "document_id": {"type": "integer"}
    }
  }
}
```
**Output**: Full-text searchable index of all document chunks

---

### **STEP 10: [STORE METADATA IN POSTGRESQL]**
**What happens**: Store structured relationships, processing status, and chunk metadata in relational database

**Why PostgreSQL**: Vector and Elasticsearch store content, but PostgreSQL stores relationships and business logic
**What gets stored**:
- Document processing status and timestamps
- Chunk hierarchy and relationships
- Cross-reference mappings
- Definition catalog
- User query history

**Database tables created**:
- `document_chunks`: Each chunk with position, section, token count
- `document_definitions`: Extracted terms and their definitions  
- `document_references`: Cross-reference mappings between sections
- `processing_jobs`: Background job status and logs

**Tools involved**: SQLAlchemy ORM, PostgreSQL database, Alembic for migrations
**Example records**:
```sql
INSERT INTO document_chunks (document_id, chunk_index, section_path, text_content, token_count, vector_id)
VALUES (123, 387, 'section_5_2', 'Section 5.2 Termination Rights...', 287, 'doc_123_chunk_387');
```
**Output**: Structured metadata enabling complex queries and relationship tracking

---

### **STEP 11: [MARK PROCESSING COMPLETE]**
**What happens**: Update document status, log processing results, notify user of completion

**Status update**: Change document.processing_status from "processing" to "completed" (or "failed" if errors)
**Metrics logged**: Processing time, number of chunks created, any errors encountered
**User notification**: Frontend polls status API or receives WebSocket notification
**Quality checks**: Verify all chunks were processed, embeddings generated, searches working

**Tools involved**: Database update, logging, WebSocket or polling for real-time updates
**Output**: Document ready for querying, user can start asking questions

---

### **‚Ä¢‚Ä¢‚Ä¢ [USER ASKS LEGAL QUESTION] ‚Ä¢‚Ä¢‚Ä¢**
**What happens**: User types question like "What are my termination rights?" in chat interface
**Frontend**: React chat component sends POST request to `/chat/query` endpoint
**Backend receives**: Question text, document ID(s) to search, user context

---

### **STEP 12: [QUERY PREPROCESSING]**
**What happens**: Analyze and prepare user question for optimal search

**Query analysis**:
- **Intent detection**: Is this about obligations, definitions, deadlines, parties?
- **Entity extraction**: Find legal terms, party names, section references in question
- **Query expansion**: Add synonyms ("termination" ‚Üí "ending", "cancellation", "expiration")
- **Legal context**: Understand this is a legal query requiring precise, cited answers

**Technical process**:
1. Extract entities using NER (Named Entity Recognition)
2. Identify legal concepts and terminology
3. Generate search variations and synonyms
4. Prepare query for different search types

**Tools involved**: spaCy NER, legal term dictionaries, query expansion algorithms
**Example preprocessing**:
- Input: "What are my termination rights?"
- Processed: ["termination rights", "termination clauses", "ending agreement", "cancellation rights"]
**Output**: Enhanced query ready for multi-round search

---

### **STEP 13: [SEMANTIC + KEYWORD SEARCH]**
**What happens**: First retrieval round combining vector similarity search and keyword matching

**Process**: 
1. Convert user question to embedding vector using same model as chunks
2. Search vector database for most similar chunks (cosine similarity)
3. Search Elasticsearch for exact phrase matches
4. Merge results from both searches, remove duplicates
5. Score chunks by relevance and rank by combined score

**Tools involved**: Vector database client, Elasticsearch client, similarity scoring algorithms
**Search APIs**:
```python
# Vector search
vector_results = pinecone.query(vector=query_embedding, top_k=20)
# Keyword search  
keyword_results = es.search(body={"query": {"match": {"text": "termination rights"}}})
```
**Output**: 10-15 relevant chunks ranked by relevance score

---

### **STEP 14: [LLM ANSWER GENERATION]**
**What happens**: Send all retrieved context and analysis to Large Language Model for comprehensive answer generation

**Prompt engineering**: Create carefully crafted prompt that includes:
- User's original question
- All retrieved and analyzed context
- Instructions for legal accuracy and citation requirements
- Format requirements for structured response

**LLM processing**: 
- GPT-4o analyzes all provided context
- Generates comprehensive answer based solely on provided information
- Includes proper legal reasoning and analysis
- Formats response with citations and structure

**Quality controls**:
- Instruct LLM to only use provided context (no hallucination)
- Request structured format with clear sections
- Ask for confidence indicators where appropriate

**Tools involved**: OpenAI API, prompt templates
**Example prompt structure**:
```
You are a legal AI assistant. Based ONLY on the provided context, answer this question: "What are my termination rights?"

Context: [All retrieved chunks and analysis]
```
**Output**: Comprehensive, cited legal answer generated by LLM

---

### **STEP 15: [RESPONSE FORMATTING]**
**What happens**: Structure the LLM-generated answer with basic citations for frontend display

**Process**:
- Parse LLM response for citation markers
- Map citation markers to chunk IDs  
- Format answer with clickable citations

**Tools involved**: Text parsing, citation mapping
**Output**: Formatted response with citations ready for frontend display

---

### **STEP 16: [FRONTEND DISPLAY WITH CITATIONS]**
**What happens**: Present the answer to user with interactive citations and document viewing capabilities

**Answer display**:
- Show main answer with inline 

citation links
- Provide expandable sections for detailed analysis
- Display search process metadata if user wants to see "how you found this"

**Citation interactivity**:
- Clickable citations that highlight exact source text
- Tooltip previews of cited content
- "Jump to document" functionality to view full context

**Document viewer integration**:
- Side-by-side document viewer showing original contract
- Automatic scrolling to cited sections when citations clicked
- Highlighting of relevant passages in original document

**Follow-up capabilities**:
- Suggested follow-up questions based on answer
- "Ask more about this section" options
- Conversation context maintained for deeper exploration

**Tools involved**: React components, PDF viewer libraries, citation linking, WebSocket for real-time updates
**User experience**: User sees comprehensive answer with full traceability back to source document
**Output**: Interactive legal answer with complete source verification

---

## üéØ **FINAL RESULT FOR USER**

**User asked**: "What are my termination rights?"

**User receives**: 
- Comprehensive answer covering all termination scenarios
- Every fact linked to specific contract sections  
- Ability to click any citation to see exact source text
- Clear explanation of conditions, deadlines, and requirements
- Confidence indicators and caveats where appropriate
- Suggestions for related questions

**Time**: 2-5 seconds instead of 30+ minutes of manual contract review
**Accuracy**: Higher than manual review due to systematic analysis
**Verifiability**: Every claim traceable to exact source location

This RAG pipeline transforms legal document analysis from manual, error-prone process into instant, systematic, verifiable answers with complete source citation.