---
description:
globs:
alwaysApply: false
---



# Legal Document QA RAG Pipeline (FastAPI) Implementation Plan

## Key Challenges in Legal RAG Pipelines

Typical retrieval-augmented generation (RAG) pipelines struggle with several legal-specific issues that must be addressed for accurate Q\&A on contracts, laws, and policies:

* **Clause Cross-References & Dependencies:** Legal clauses often refer to other clauses or sections for full context. A single-pass RAG query may miss information hidden behind these internal references. For example, if *Clause 7.2* says “Refer to Clause 7.3 and 7.4,” a basic pipeline might ignore 7.3/7.4 entirely, leading to incomplete answers. Cross-clause dependencies require special handling beyond typical semantic search.
* **Defined Term Resolution:** Legal documents contain a “Definitions” section that assigns precise meanings to capitalized terms (parties, roles, acronyms, etc.). Out-of-the-box RAG usually doesn’t automatically pull in these definitions, so answers may be incorrect or lack the exact intended meaning of terms. Resolving defined terms to their formal definitions is essential for accurate understanding (e.g. knowing “CCO” means *Chief Compliance Officer* per the document’s definition).
* **Numerical Inconsistencies:** Contracts often include numbers (dates, amounts, timeframes) that need consistency across clauses. A standard pipeline might answer a question using one clause’s number and fail to note a conflicting number elsewhere. Research shows RAG systems can overlook altered or inconsistent numerical values between sections. For instance, if one clause says *30 days* and another says *60 days* for the same obligation, a basic RAG won’t catch the discrepancy without explicit checks.
* **Obligation Scope & Responsibility Ambiguity:** Identifying *who* has to do *what* (and under what conditions) in legal text is tricky. Even advanced multi-round RAG struggled with *obligation scope detection* and responsibility errors. A naive QA system might miss clauses that limit or condition an obligation (e.g. “Party A shall do X **unless** Y happens”) or fail to clarify which party a duty applies to. This can lead to answers that are not fully context-grounded about duties and exceptions.
* **Structural Complexity (Nested Clauses, Footnotes, External References):** Legal documents are highly structured (sections, subsections, annexes) with critical details sometimes in footnotes or appendices. Important caveats might reside in a footnote or an attachment, which a typical RAG (focused on main text) might ignore. Similarly, contracts may reference external laws or documents. Without handling these structural and external references, the QA system may miss context or provide non-comprehensive answers.
* **Domain-Specific Terminology & Repetitive Language:** Legal queries often use formal terms that must match the document text exactly. Pure semantic embeddings can sometimes miss exact matches for uncommon legal terms or over-emphasize semantically similar but irrelevant text. For example, a dense retriever might overlook a clause because the wording is technical or the term is rare. Conversely, legal text can be verbose and repetitive, requiring precise keyword matching for certain answers. Typical RAG pipelines that rely only on vector similarity may have recall issues in this domain. Using just one retrieval method can result in missed relevant clauses or inclusion of tangential ones.

## Enhancements for the RAG Pipeline

To tackle the above challenges, we recommend extending the FastAPI-based QA pipeline with additional modules and logic. The FastAPI service can orchestrate these steps in sequence to produce citation-grounded answers. Key enhancements include:

### 1. Clause Reference and Hierarchy Navigation

* **Document Structure Parsing:** Upon document ingestion, parse and store the legal document’s hierarchy (sections, clauses, sub-clauses, etc.). Each clause/section should be identifiable by an ID or heading (e.g., *Section 7.4(b)* or *Article 5.2*). This can be done by regex or an outline parser (many contracts number their paragraphs). Building this *lexical graph* of the document structure ensures the pipeline knows the relationships between clauses.
* **Clause Reference Detection:** Enhance the QA logic to detect references to other parts of the document in both the user question and the retrieved passages. This means scanning text for patterns like “Section X”, “Clause Y.Y”, “Paragraph Z” etc. If an initial retrieval result or user query mentions another clause, automatically lookup that clause via the structured index. For example, if the user asks about obligations “subject to clause 5”, the system should fetch Clause 5 as well. This addresses the gap where a basic semantic search would not automatically fetch referenced clauses.
* **Recursive Retrieval of Linked Sections:** Implement an iterative loop that follows cross-references. After the initial retrieval, the pipeline should **recursively** retrieve any clause that was mentioned in the found text (and then clauses mentioned within those, and so on). In practice, after answering agent gathers initial context, a “clause traversal” function can collect all referenced IDs, retrieve those chunks from the index, add them to the context, and repeat until no new references are found or a round limit is reached. This ensures full context is gathered from scattered provisions (e.g. if Clause 7.2 points to 7.3 and 7.4, which then point to 9.1, the pipeline will retrieve 7.3, 7.4, and 9.1 in subsequent rounds).
* **Footnote & Attachment Handling:** Similarly, include logic for footnotes or endnotes. If a clause contains a footnote reference (e.g. superscript number or “\[See Footnote 3]”), the pipeline should retrieve the footnote text. Parse the footnote for any clause references or important details and incorporate that into the context. This prevents loss of critical information often residing in footnotes or annexes. The FastAPI service might maintain a separate index for footnotes/appendices or handle them as part of the clause graph.
* **Hierarchy-Aware Reasoning:** Using the hierarchical map, the system can also understand clause *contexts* (e.g., knowing Clause 5.2 is under Section 5 helps interpret it). If needed, the pipeline can be configured to pull sibling or parent clauses for additional context (for instance, the title of Section 5 might give context to Clause 5.2’s meaning). This hierarchy awareness aids the language model in disambiguating references like “the above requirements” or “in this section.”

### 2. Definition Resolution Module

* **Extract and Index Definitions:** During document ingestion, identify if the document has a “Definitions” or “Interpretation” section (commonly at the start or end of contracts). Extract each term and its definition (e.g., using regex for terms in quotes or a prompt to an LLM to find term-definition pairs). Store these in a **definitions index** (could be a simple dictionary or a separate vector index) keyed by the term. Each definition entry should know its location in the document for citation.
* **Query-Time Definition Lookup:** Before running the main retrieval, parse the user’s question for any capitalized or defined terms. If the question contains a term like “CCO” or “Effective Date” that matches an entry in the definitions index, fetch that definition text. Similarly, after initial clause retrieval, scan the retrieved passages for capitalized terms defined in the document. Any relevant definitions should be fetched and added to the context prior to answer generation. This ensures the LLM is provided the official meaning of key terms, resolving ambiguities (for example, knowing that “CCO” stands for *Chief Compliance Officer* as defined in the document).
* **In-Context Definition Injection:** Merge the retrieved definitions into the prompt context (e.g., prepend a section “Relevant Definitions:” listing the term and exact definition text). This gives the model the exact language of the definition to quote or rely on. The definitions module can be implemented without fine-tuning – either by straightforward text extraction or a one-time LLM-assisted parsing of the definitions section (which was done in the referenced implementation using GPT-4 to output a structured list of terms). By handling definitions explicitly, the pipeline avoids misinterpretation of specialized terms and can answer questions about, say, “What does X mean in this contract?” directly from the source.

### 3. Iterative Multi-Round Retrieval

* **Progressive Query Refinement:** Integrate a multi-round retrieval mechanism to gather context in stages. Instead of a single vector database query, the pipeline should support iterative rounds where the query or retrieval criteria are refined based on what was found previously. For example, the first round might retrieve top relevant clauses; then the system checks those results for missing pieces (unresolved references, ambiguities, or hints that more context is needed) and formulates a more specific follow-up query. This process repeats until the query is sufficiently answered with comprehensive context or a set round limit is reached.
* **Automated Stopping Criteria:** Implement logic to decide when to stop retrieving more rounds. The pipeline can stop when no new clause IDs or relevant info are found in the latest round, or when adding more context stops improving the answer. A reasonable cap (e.g. 2–3 rounds) can prevent infinite loops. Each round’s results should be accumulated so the LLM sees the full set of relevant snippets.
* **Improved Recall for Cross-References:** This iterative approach significantly increases the recall of relevant clauses compared to single-pass retrieval. By dynamically expanding the context, the system catches dependencies that a one-shot query misses (the multi-round method *“halves missed cross-clause dependencies”* in experiments). In practice, this means fewer oversight of clauses that are conditionally linked or spread out in the document. The trade-off is some loss in precision by pulling slightly more material, but for legal QA, completeness is often worth it.
* **FastAPI Orchestration:** Within a FastAPI endpoint, this could be implemented as a loop that runs retrieval, updates a “context set,” and checks a condition to continue. Each iteration can log what new clauses were added. This logic can reside in an application service layer, using the vector database’s client to issue new searches (for example, searching by newly discovered clause references or using the LLM to suggest a refined query). No model fine-tuning is needed – this is an orchestration improvement leveraging the existing retriever and possibly the LLM for query suggestions.
* **Example – Multi-Round in Action:** If the user asks, *“What are the Board’s obligations regarding the CCO’s role?”*, round 1 might retrieve a clause about Board duties and one about CCO. That clause might say “subject to Section 7.4(b)”. The pipeline then searches specifically for Section 7.4(b) in round 2, finds it (plus maybe it references Section 9.1), and then round 3 retrieves Section 9.1. The final context fed to GPT includes all these pieces, enabling an answer that correctly cites 7.4(b) and 9.1 in addition to the main clauses – something a single-pass system would likely miss.

### 4. Hybrid Dense + Keyword Retrieval

* **Combine Semantic and Lexical Search:** Use a **hybrid retrieval strategy** to improve coverage of relevant text. Rely on vector similarity for broad semantic matching **and** incorporate a keyword or BM25 search for exact term matching. In implementation, this could mean querying both the vector database and a simple full-text index (e.g., Elasticsearch or even an in-memory index of chunks) with the user’s question. Merge the results from both before filtering/ranking.
* **Why Hybrid:** Legal text often requires high precision on specific terminology. The dense vector search might miss a clause if the query uses synonyms or if the relevant clause uses unique legal verbiage. Conversely, BM25 will catch passages containing the exact keywords of the query (or closely related terms), which is crucial if the question uses statutory or domain language. As noted, adding BM25 helps *“identify key terms in highly repetitive text,”* and a keyword retriever ensures even infrequent but important terms are found. This addresses cases where the query term appears only in one clause; a dense model might not always rank that clause highest if the semantics are subtle.
* **Implementation:** For example, if the user asks *“What is the definition of Force Majeure?”*, a BM25 search will directly find the definition clause containing “Force Majeure,” whereas a purely semantic search might return related concepts but not the exact definition. Similarly, if the query is *“termination fee”* and the contract consistently uses the phrase “termination charge,” a semantic search should catch it, but the keyword search will guarantee we don’t miss it. By combining both, the pipeline improves recall without sacrificing relevance.
* **Ranking and Deduplication:** Merge results by perhaps taking top N from each method and removing duplicates (since dense and sparse may return overlapping chunks). Use a simple heuristic or even let the LLM weigh the importance (feeding slightly more context is fine as long as it’s relevant). Many RAG frameworks (like LlamaIndex or LangChain) support hybrid retrieval out-of-the-box, or you can manually implement it by querying the vector store and a TF-IDF/BM25 index separately. No model fine-tune is needed – this is about utilizing multiple retrievers for robustness.
* **Metadata Filters for Precision:** Additionally, take advantage of metadata in retrieval. For instance, if the query explicitly mentions *“Clause 10”*, use a metadata filter or direct lookup to fetch Clause 10 (since we stored clause numbers in the index). If the question implies a certain section (e.g. “in the termination provisions…”), consider filtering or boosting results from that section. This kind of lexical filtering works well in tandem with semantic search, ensuring the right *location* is retrieved when explicitly specified by the user.

### 5. Numerical Consistency Checks

* **Post-Retrieval Numeric Scan:** Add a post-retrieval step to detect and handle numerical information in the gathered context. The pipeline should scan the retrieved clauses for any numbers, dates, or other quantifiable terms (e.g. deadlines in days, monetary amounts, percentages). If the user’s question is about a specific number (“What is the penalty amount?”) or timeframe (“How long is X effective?”), ensure that the answer is validated against all relevant mentions. For example, if Clause 5.2 says a 60-day period and Clause 7.1 mentions a 30-day period on the same topic, the system should flag this discrepancy. Such inconsistencies are often missed by single-pass QA, so our pipeline will proactively check for them.
* **Strategy to Implement:** One approach is to identify the *topic* of the query (e.g., “payment timeline”) and then specifically search the document for all numeric values related to that topic. However, a simpler general approach is: after retrieving context, use a script or even the LLM itself to look for multiple different numbers that might conflict. For instance, if two retrieved chunks both mention a number for a similar concept (like two clauses both talk about termination fees but list different amounts), have the pipeline note this. We could prompt the LLM in a second pass: “Given the extracted clauses, are there any conflicting numbers or dates regarding this issue?” to leverage its ability to spot inconsistencies.
* **No Fine-Tuning Required:** This can be done with basic Python checks (regex for digits) plus some heuristic mapping of what those numbers refer to (maybe by reading the surrounding text). If more sophistication is needed, one can use an LLM to analyze consistency. The key is that the final answer should reflect awareness of numerical consistency. For example, the answer might say, *“Clause 4.2 sets a 30-day period, but Clause 7.1 specifies 60 days for the same requirement.”* If the goal is strictly QA and not analysis, at minimum ensure the **correct** number is cited from the authoritative clause. If the question is open-ended (like “are there any inconsistencies”), the pipeline can surface these findings explicitly.
* **Use Case – Date/Amount Verification:** If asked “When does the contract end?”, and the contract has a date in words in one place and a numeric date in another, the pipeline should retrieve both if relevant and clarify which is controlling. A numeric check module could catch if one clause says “December 31, 2025” and another erroneously says “2026” due to an amendment. While the LLM can answer with one, a verification step ensures it’s citing the correct one or notes the discrepancy. This kind of consistency check adds reliability for legal QA, where such details are critical.

### 6. Obligation Scope & Conditional Logic Handling

* **Party-Obligation Mapping:** Introduce a lightweight **obligation extraction** mechanism to help clarify “who is responsible for what” in the retrieved text. Often, legal QA might ask something like “What are Party A’s obligations under the agreement?” A generic RAG answer might list obligations but miss conditions or scope. We can improve this by parsing clauses for subjects and modal verbs (e.g., “Party A shall…” vs “Party B must…”). A simple rule-based NLP pass or regex can identify which party or actor is associated with each obligation in the text. This metadata can then be used to filter or organize the answer (e.g., group obligations by party).
* **Conditional Clauses:** If clauses contain conditional language (“if X, then Y” or “unless Z”), ensure that context isn’t lost. The pipeline should treat a clause with conditions as a single chunk if possible, so the LLM sees the full condition. Additionally, the answer generation prompt can remind the model to include relevant conditions or exceptions. For instance, if a clause obligation only applies *“except during holidays”*, the answer should reflect that nuance. By maintaining clause integrity in chunking, as noted below, we preserve these logical conditions.
* **Explicit Scope Prompting:** When forming the prompt for the LLM, include a cue to consider *obligation scope*. For example: *“Present the obligations and note any party responsible and any exceptions.”* This can be part of the system or user prompt template. The academic approach of assigning a “legal expert” role and structured output is instructive here – they ensure *“minor clauses and subtle definitions receive the same scrutiny as major provisions”*. In our pipeline, we similarly instruct the model to be thorough with responsibilities and conditions.
* **Addressing Missed Obligations:** If the evaluation of the pipeline shows certain obligation-related errors (the research noted zero improvement in some responsibility errors in one configuration), we might add a fallback: a list of common obligation keywords (“shall”, “must”, “responsible for”, “duty to”) to ensure any clause with these terms gets into the retrieval when a responsibility question is asked. Essentially, if the query is about obligations or responsibilities, bias the retrieval to include all clauses containing “shall” or “must” pertaining to the relevant party. This can be done via an OR query in BM25 or a second targeted search.
* **Example:** For a question like “What must the contractor do if the project is delayed?”, apart from retrieving the main force majeure or delay clause, the pipeline should also fetch any clause that defines the contractor’s responsibilities generally (maybe in a roles section) and any clause that mentions exceptions (perhaps an extension clause). By preemptively including these, the answer can cite all pertinent obligations and conditions, giving a full picture rather than a narrow answer.

### 7. Prompting and Answer Formulation Enhancements

* **Legal Expert Persona & Instructions:** Use the system prompt to prime the OpenAI model as a legal analyst. For instance: *“You are a legal expert answering questions about a contract. Only use the provided document excerpts to answer. If the answer depends on specific clauses or definitions, cite them.”* Setting the tone helps ensure the model sticks to the text and uses a formal style appropriate for legal content. It reduces hallucinations by emphasizing that answers must be *grounded in the given sources*.
* **Structured Answer Format:** Encourage the model to output answers in a structured way (headings or bullet lists) especially when multiple points or obligations are involved. Legal answers often benefit from enumeration (e.g., list of obligations, list of conditions). The pipeline can include an instruction like: *“Format the answer as a list of points, each with a reference to the clause.”* This not only makes the answer more readable (short paragraphs and bullet points as per the user’s preference) but also inherently embeds citations per point. The research even utilized a JSON output schema for consistency – while we may not require JSON, the idea is to have a clear, itemized answer for verification.
* **Source Attribution in Answers:** The LLM should explicitly refer to the clause numbers or section titles in its answer text. We will instruct it to do so – for example: “(Clause 7.4(b))” or “as per Section 9.1”. This is in addition to the formal citation brackets we attach. The Medium example pipeline’s answering agent prompt says *“Give references to sections/paragraphs if possible”*. In practice, when constructing the final prompt with retrieved context, include a line like: *“Always cite the clause or section where each answer piece comes from.”* This ensures the model’s generated text already contains in-line references (which our system can later map to full citations if needed).
* **Preventing Over-Reliance on LLM Memory:** Only supply the model with retrieved context and avoid broad questions without context. The pipeline should *not* rely on the LLM’s internal knowledge for specific legal content – every factual claim should come from a retrieved snippet. This can be enforced by a prompt that says “If you are unsure or the information is not in the provided excerpts, say you don’t know.” It’s better to have the model admit uncertainty (or trigger a refinement loop) than fabricate an answer in legal settings. By structuring the FastAPI workflow to always perform retrieval first and feed results into the OpenAI completion API, we ground the model’s knowledge on the actual document.
* **Example Prompt Structure:**

  ```
  SYSTEM: "You are a legal Q&A assistant. Answer the question using the given document excerpts. Cite clause numbers for each fact, and do not add information not found in the text."  
  USER: "<Question text>\n\nDocument Excerpts:\n- [Clause 6.3(a)] ...text...\n- [Clause 7.2] ...text...\n- [Definition of 'CCO'] ...text..."  
  ASSISTANT: (expected answer with citations)
  ```

  This way, the model sees a curated set of excerpts (including any definitions, related clauses from multi-round retrieval, etc.) and is explicitly guided to use them for the answer. The output will be a well-structured answer referencing the clause numbers, which the pipeline can post-process into the desired citation format.

## Document Chunking and Indexing Strategy

A careful chunking and indexing approach will ensure the retriever can find relevant information and that citations map back correctly:

* **Clause-Level Chunking:** Split the legal document into chunks that align with the natural boundaries of the text – ideally each chunk represents a single clause, section, or paragraph that constitutes a cohesive idea. Using the document’s structure (headers, numbering) is key. For example, treat *Section 5.1*, *5.2*, *5.3* each as separate chunks if they’re reasonably sized. This *section-based chunking* preserves semantic coherence and keeps context intact. It also aligns with how lawyers cite documents (by clause/section), which makes citation tracking easier. In the academic study, documents were chunked by “natural portions” (sections) up to \~512 tokens to balance context and efficiency – that is a good rule of thumb (for English text, \~500 tokens \~ 350 words, which is usually a long clause or a short section).
* **Avoid Cutting Key Sentences:** Ensure chunk boundaries don’t split a sentence or logical condition across chunks. Each chunk should be self-contained enough that if it’s retrieved alone, it makes sense. If a clause is extremely long (spans multiple paragraphs or pages), consider splitting it into sub-chunks (e.g., by sub-clause lettering or bullet points) but **include an overlap or indicator** so that context isn’t lost. It might be beneficial to include the clause heading in all sub-chunks as metadata or prefixed text (e.g., “Clause 12.5 (continued): …”) to remind the model that those chunks are related.
* **Metadata for Each Chunk:** Store rich metadata with each chunk in the vector database. At minimum include: the document ID/name, section or clause number, and maybe the page number or paragraph number in the original document. For example, a chunk’s metadata could be `{"section": "7.4(b)", "title": "Separation from Internal Audit", "page": 12}`. This metadata will be used to formulate citations (e.g., citing “Section 7.4(b) of the Policy Document”) and can also aid retrieval filtering (“find section==7.4 if mentioned”). Metadata is inexpensive to store and invaluable for traceability.
* **Index Definitions and Footnotes Separately:** As mentioned, treat the Definitions section as its own mini-index or at least mark those chunks with `type: "definition"` in metadata. Similarly, footnotes or annexes can be tagged. This way, the retrieval step can target definitions specifically when needed (e.g., the Definition Module can query `type:definition` index) without them being drowned out by main body text. It also means when a definition is retrieved, we know it’s a definition and can present it differently (the answer might cite it as “Definition of X”). If using a single vector store for all text, ensure to include the section name (like “Definitions”) in the chunk text or metadata so those entries are easily distinguishable.
* **Use of Multiple Indices (Optional):** For a complex legal corpus, you might use multiple indexes: one for the primary document text, one for statutory/reference materials, one for definitions, etc. In a simple FastAPI setup, it might be easiest to combine everything with metadata and use filtering. But separating can improve retrieval focus (the pipeline chooses which index to query based on the context — e.g., user asks a regulatory question, query the law database as well). The academic approach, for instance, had a retrieval dataset of external laws and regs to check contract clauses. If your use case might require referencing external laws, plan to maintain an updated index of those as well.
* **BM25/Keyword Index:** In addition to the vector index, maintain a lightweight keyword index. This could be as simple as storing all chunk texts in an Elasticsearch or using Whoosh/Lucene to allow keyword queries. Alternatively, LlamaIndex or similar can internally use a combined vector+BM25 retriever as they did. The indexing step should compute embeddings for each chunk (for vector search) and also feed the chunk text into the keyword index. Ensure the unique ID of chunks is consistent across both so that when you retrieve from either, you can identify the same source chunk.
* **Chunk ID Scheme:** Assign each chunk a stable identifier, e.g., “DocName-Section5.2” or a GUID. This ID can be used to retrieve the full text or map to the original document location when preparing citations. If possible, encode the section number in the ID for readability. The FastAPI service might have an endpoint or function to retrieve the original document snippet by ID for verification or for sending to the client if needed.
* **Precomputing Clause Links:** As part of indexing, it’s beneficial to precompute a mapping of clause references: e.g., analyze each chunk text for “Clause/Section X” references and store those relationships (this forms a graph of references). While not strictly necessary, it can speed up the recursive retrieval – instead of searching the vector DB for “Clause 9.1”, you could directly jump to the chunk with ID “9.1” via this mapping. If the document is well-structured, this mapping is essentially the adjacency list of the clause graph. In practice, one could create a dictionary like `references = { "7.2": ["7.3", "7.4"], "7.4(b)": ["9.1"], ... }` from a preprocessing script. The Router/Recursive agent in the Medium example effectively uses such a map. In a FastAPI pipeline, this could be a cached object in memory to consult for quick retrieval of linked sections.

## Citation Tracking and Answer Grounding

To ensure answers always cite the correct location in the source document, implement robust citation tracking and formatting in the pipeline:

* **Metadata-to-Citation Mapping:** Leverage the metadata stored with each chunk to generate citations in the answer. For instance, if a chunk has metadata `{"section": "5.2", "doc": "Master Service Agreement", "page": 12}`, the pipeline can format a citation like “(Master Service Agreement §5.2)” or simply “(Section 5.2)” if the context of document is clear. Decide on a citation style that is concise but unambiguous. Clause/section numbers are ideal because they are how lawyers pinpoint text. The pipeline should programmatically insert these references when finalizing the answer.
* **In-Answer Citations vs Post-Processing:** There are two ways to attach citations: have the LLM include them (guided by prompt) or add them afterward. We suggest a hybrid approach: instruct the LLM to mention clause numbers in parentheses (which it will do as per our prompt design), and then the pipeline can wrap those in whatever markup or format needed for the end user. Since the user specifically mentions citation-grounded answers, the answer should read like, *“The Board must approve the CCO’s appointment (Section 6.3(a)),”* making it obvious where it came from. This practice was demonstrated in the multi-agent example, where the answer lines included references like “(S 6.3(a), (b))” inline.
* **Verify Citation Accuracy:** Implement a safeguard that every fact the LLM outputs has a source. The FastAPI service can parse the answer for any clause/section mentions and cross-check them against the retrieved context. If the model output cites a clause that wasn’t retrieved (or doesn’t exist), the pipeline can either remove that part or flag it as potential hallucination. Essentially, the only sections that should appear in the answer are those the pipeline provided to the model (plus maybe definitions). This is a sanity check to keep the system honest.
* **Always Provide Source Snippets:** Configure the final answer payload to include not just the answer text but also the cited source snippets (or at least their references) for transparency. For example, the FastAPI response could contain the answer string and a list of citations with clause number and text excerpt. This helps users quickly verify the answer against the document. It’s also useful for debugging – you can see exactly which chunks were used.
* **Citation Format Best Practices:** Use a consistent, clear format for citations in the answer. Common approaches:

  * Inline parentheses with clause/section (and maybe document abbreviation if multiple docs). E.g., “(Clause 4.5)” or “(Policy §4.5.2)”.
  * Superscript footnote numbers that correspond to a reference list after the answer. E.g., “...as required by the contract¹.” Then “¹ Section 10.2 of Contract ABC”. This is more formal, but it’s harder for a user to parse without looking at footnotes. Inline is more straightforward in interactive QA.
    Given the user’s requirement, using the clause/section in-text seems preferable. It ensures the answer is self-contained and immediately verifiable.
* **Track Source Throughout the Pipeline:** Maintain the association between retrieved text and its source ID all the way through the generation step. For instance, if you stuff multiple retrieved chunks into the prompt, you might prepend each with a reference tag (like `[6.3(a)] ... text ...`). The LLM can then use these tags when formulating the answer. This technique was hinted in the Medium example, where the answering agent was given a list of document nodes and asked to link to sources by paragraph number. By giving each snippet a label (the section number), we make it easy for the model to quote or reference it in the answer. The final answer can then replace those labels with a more readable citation if necessary.
* **Example of Citation Tracking:** Suppose the retrieved context for a question includes: *Section 3.1: “The Tenant shall maintain insurance...”* and *Section 7.2: “Landlord may terminate the lease if...”*. These are passed into GPT-4 with identifiers. The model might produce an answer: “Yes, the tenant is required to maintain insurance (Section 3.1), and failure to do so gives the landlord a right to terminate the lease (Section 7.2).” The pipeline would then ensure that “Section 3.1” and “Section 7.2” in the answer correspond to the actual document’s Section 3.1 and 7.2. Because we labeled and provided those sections, the model is citing them correctly. We then deliver the answer with those citations intact. This approach guarantees each answer sentence is grounded in the source.
* **Addressing Multi-Document Scenario:** If the pipeline might handle multiple documents (say a corpus of laws plus a contract), include document identifiers in citations to avoid confusion (e.g., “Contract X §5.1” vs “Law Y §10”). The metadata should have document names. In a single-document QA, mentioning the clause number alone is sufficient.
* **Logging and Monitoring:** It’s good practice for the FastAPI service to log which chunks were retrieved and which ended up being cited in the answer. This can help refine the process if you find that certain relevant clauses were missed or if the model is citing something irrelevant. Monitoring the occurrences of “I don’t know” or hallucinations in answers (which ideally should be zero with these constraints) will tell you if the retrieval pipeline needs tweaks.

By implementing the above modules and strategies, the FastAPI-based RAG pipeline will be **much better equipped to handle legal documents**. It will capture clause dependencies and definitions that a naive approach would miss, use iterative retrieval to gather complete context (improving recall of scattered obligations), and produce answers that are **grounded in the source text with precise citations**. These enhancements directly address the gaps identified in recent research – ensuring that even without fine-tuning a model, we leverage intelligent retrieval and prompting to meet the exacting standards of legal QA. The end result will be a robust QA service that can answer questions about contracts, laws, or policies with a high degree of confidence and traceability, informing users *“according to Clause X…”* with evidence, rather than just generically.
