# Document Processing Services

This module contains all services for processing legal documents in the RAG pipeline. The services are organized into specialized modules that handle different aspects of document processing.

## ğŸ“ Directory Structure

```
document_processing/
â”œâ”€â”€ __init__.py                 # Main module exports
â”œâ”€â”€ requirements.txt            # Dependencies for all services
â”œâ”€â”€ README.md                  # This documentation
â”‚
â”œâ”€â”€ pipeline/                  # Main orchestration service
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base.py               # Main pipeline orchestrator
â”‚   â”œâ”€â”€ background_processor.py
â”‚   â”œâ”€â”€ status_tracker.py
â”‚   â”œâ”€â”€ error_handler.py
â”‚   â”œâ”€â”€ job_queue.py
â”‚   â””â”€â”€ workflow_manager.py
â”‚
â”œâ”€â”€ text_extraction/          # Text extraction from files
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base.py              # Main extraction service
â”‚   â”œâ”€â”€ pdf_extractor.py     # PDF text extraction
â”‚   â”œâ”€â”€ docx_extractor.py    # Word document extraction
â”‚   â”œâ”€â”€ ocr_extractor.py     # OCR for scanned documents
â”‚   â””â”€â”€ text_extractor.py    # Plain text files
â”‚
â”œâ”€â”€ structure_analysis/       # Document structure analysis
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base.py              # Main structure analyzer
â”‚   â”œâ”€â”€ legal_structure_analyzer.py
â”‚   â”œâ”€â”€ section_extractor.py
â”‚   â””â”€â”€ hierarchy_builder.py
â”‚
â”œâ”€â”€ legal_extraction/         # Legal content extraction
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base.py              # Main legal extractor
â”‚   â”œâ”€â”€ definitions_extractor.py
â”‚   â”œâ”€â”€ cross_reference_extractor.py
â”‚   â”œâ”€â”€ obligations_extractor.py
â”‚   â”œâ”€â”€ parties_extractor.py
â”‚   â””â”€â”€ dates_extractor.py
â”‚
â”œâ”€â”€ chunking/                 # Document chunking services
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base.py              # Main chunking service
â”‚   â”œâ”€â”€ semantic_chunker.py
â”‚   â”œâ”€â”€ legal_chunker.py     # Legal-specific chunking
â”‚   â”œâ”€â”€ overlap_manager.py
â”‚   â””â”€â”€ chunk_metadata.py
â”‚
â”œâ”€â”€ embedding/                # Vector embedding generation
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base.py              # Main embedding service
â”‚   â”œâ”€â”€ openai_embeddings.py
â”‚   â”œâ”€â”€ huggingface_embeddings.py
â”‚   â”œâ”€â”€ embedding_cache.py
â”‚   â””â”€â”€ batch_processor.py
â”‚
â”œâ”€â”€ vector_store/             # Vector database operations
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base.py              # Main vector store service
â”‚   â”œâ”€â”€ pinecone_store.py
â”‚   â”œâ”€â”€ weaviate_store.py
â”‚   â”œâ”€â”€ chroma_store.py
â”‚   â”œâ”€â”€ pgvector_store.py
â”‚   â””â”€â”€ similarity_search.py
â”‚
â”œâ”€â”€ search_indexing/          # Full-text search indexing
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base.py              # Main search service
â”‚   â”œâ”€â”€ elasticsearch_indexer.py
â”‚   â”œâ”€â”€ legal_analyzer.py
â”‚   â”œâ”€â”€ boolean_search.py
â”‚   â”œâ”€â”€ hybrid_search.py
â”‚   â””â”€â”€ index_manager.py
â”‚
â”œâ”€â”€ config/                   # Configuration management
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base.py
â”‚   â”œâ”€â”€ embedding_config.py
â”‚   â”œâ”€â”€ vector_store_config.py
â”‚   â”œâ”€â”€ search_config.py
â”‚   â”œâ”€â”€ chunking_config.py
â”‚   â””â”€â”€ extraction_config.py
â”‚
â””â”€â”€ utils/                    # Shared utilities
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ text_utils.py
    â”œâ”€â”€ legal_patterns.py
    â”œâ”€â”€ file_utils.py
    â”œâ”€â”€ metadata_utils.py
    â””â”€â”€ logging_utils.py
```

## ğŸ”„ Processing Pipeline Flow

The document processing follows this sequence:

1. **Text Extraction** (`text_extraction/`)

   - Extract raw text from PDF, DOCX, or other formats
   - Handle OCR for scanned documents
   - Preserve formatting and structure information

2. **Structure Analysis** (`structure_analysis/`)

   - Identify document hierarchy (sections, subsections)
   - Extract headers, footers, and legal patterns
   - Build document structure tree

3. **Legal Content Extraction** (`legal_extraction/`)

   - Extract definitions and defined terms
   - Find cross-references between sections
   - Identify legal obligations and parties
   - Extract dates, deadlines, and amounts

4. **Document Chunking** (`chunking/`)

   - Split document into semantically meaningful chunks
   - Preserve legal context and section boundaries
   - Add metadata for citations and references

5. **Embedding Generation** (`embedding/`)

   - Convert text chunks to vector embeddings
   - Support multiple embedding models (OpenAI, HuggingFace)
   - Batch processing for efficiency

6. **Vector Storage** (`vector_store/`)

   - Store embeddings in vector database
   - Support multiple backends (Pinecone, Weaviate, etc.)
   - Enable similarity search capabilities

7. **Search Indexing** (`search_indexing/`)
   - Index chunks in Elasticsearch for keyword search
   - Configure legal-specific text analysis
   - Enable hybrid search (semantic + keyword)

## ğŸ¯ Key Services

### DocumentProcessingPipeline

Main orchestrator that coordinates all services and manages the processing workflow.

### TextExtractionService

Handles text extraction from various file formats with fallback to OCR for scanned documents.

### LegalContentExtractor

Specialized service for extracting legal-specific content like definitions, obligations, and cross-references.

### LegalChunkingService

Intelligent chunking that preserves legal meaning and maintains proper context for citations.

### VectorStoreService

Manages vector database operations with support for multiple backends and similarity search.

### SearchIndexingService

Handles full-text search indexing with legal-specific analyzers and hybrid search capabilities.

## ğŸ”§ Configuration

Each service has its own configuration module in `config/` that manages:

- API keys and credentials
- Processing parameters (chunk sizes, overlap, etc.)
- Model selection and settings
- Environment-specific configurations

## ğŸ“š Dependencies

See `requirements.txt` for the complete list of dependencies. Key categories include:

- **Text Extraction**: PyPDF2, pdfplumber, python-docx, pytesseract
- **NLP**: spaCy, transformers, sentence-transformers, langchain
- **Vector Databases**: pinecone-client, weaviate-client, chromadb
- **Search**: elasticsearch, opensearch-py
- **Background Jobs**: celery, redis, rq

## ğŸš€ Usage

```python
from services.document_processing import DocumentProcessingPipeline

# Initialize the pipeline
pipeline = DocumentProcessingPipeline()

# Process a document
result = await pipeline.process_document(
    document_id=123,
    s3_url="https://bucket.s3.amazonaws.com/document.pdf"
)

# Check processing status
status = await pipeline.get_processing_status(document_id=123)
```

## ğŸ§ª Testing

Each service module should include comprehensive tests covering:

- Unit tests for individual components
- Integration tests for service interactions
- End-to-end tests for complete workflows
- Performance tests for large documents

## ğŸ“Š Monitoring

The services include built-in monitoring and logging:

- Processing status tracking
- Performance metrics
- Error handling and reporting
- Progress updates for long-running operations

## ğŸ”„ Background Processing

The pipeline supports asynchronous background processing using:

- Celery for distributed task processing
- Redis for message queuing
- Status tracking for real-time updates
- Error handling and retry logic

This architecture ensures scalable, reliable document processing for the legal AI system.
